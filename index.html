<html>
<head>
<title> Alekh Agarwal </title>
<script type="text/javascript" src="spam.js"></script>
<META NAME="keywords" CONTENT="alekh, alekh agarwal, Alekh, Alekh Agarwal">
</head>
<body bgcolor="#fffff">
<table width=990 align=center bgcolor="#FFEFD5"r><td width=50 align=left><img src="alekh.jpg"><br><br></td>
<td width=600 align=left><font face=helvetica><h2>Alekh Agarwal</h2><b>Staff Research Scientist</b><br>Google<br>
    <b>Email:</b> <script>spam("alekhagarwal",1);</script>, <script>spam("alekhagarwal",2);</script><br><br>
</td></tr><br>
</table>

<table width=990 align=center><tr>
    <table width=990 align=center><tr><td width=100 align="left"><br><font size=+2 face="helvetica">About Me</font></td></tr><tr></table><hr width=990 align="center"></tr>
<tr><table width=990 align=center><tr><td width=600 align="left"><font face="helvetica">I am currently a researcher in the learning theory team at Google. Prior to that, I spent nine wonderful years at Microsoft Research where I was a member of the Machine Learning group in the New York lab and later led the Reinforcement Learning team in Redmond. I obtained my PhD in Computer Science from UC Berkeley, working with  <a href="http://www.cs.berkeley.edu/~bartlett">Peter Bartlett</a> and <a href="http://www.cs.berkeley.edu/~wainwrig">Martin Wainwright</a>.</font></td></tr>


<tr><td width=600 align="left"><br><font face="helvetica"><font size=+2>Interests</font>
<hr width=990 align="center">
I am broadly interested in <b>Machine Learning</b>, <b>Statistics</b> and <b>Optimization</b>. I am currently working on several aspects of Interactive Machine Learning, including contextual bandits, reinforcement learning and active learning with an eye towards practical learning systems with strong theoretical guarantees. I have previously worked on tradeoffs between computational and statistical complexities, large-scale and distributed machine learning and statistical inference in high-dimensions.
<br><br>
</tr>
<tr><td width=600 align="left"><br><font face="helvetica"><font size=+2>Publications</font>
<a name="publications"></a>
<hr width=990 align="center">
<br>
<b>RL theory monograph:</b> A <a href="https://rltheorybook.github.io">monograph </a> on RL theory based on notes from courses taught by Nan Jiang at UIUC and together with Sham Kakade at UW. The notes are being actively updated, and any feedback, typos etc. are welcome.<br><br>
<font face="helvetica"><font size=+2>Ph.D. Thesis</font><br>
<ul>
<li><a href="thesismain.pdf">Computational Trade-offs in Statistical Learning</a>, Ph.D. Thesis, Department of Computer Science, UC Berkeley, 2012. <br><br>
</ul>
<font face="helvetica"><font size=+2>Recent preprints</font>
  <ul>
    <li><a href="https://arxiv.org/abs/2102.07035">Model-free Representation Learning and Exploration in Low-rank MDPs</a><br>with Aditya Modi, Jinglin Chen, Akshay Krishnamurthy and Nan Jiang.<br><br></li>
  <li><a href="https://arxiv.org/abs/2003.12880"> Federated Residual Learning </a><br> with Chen-Yu Wei and John Langford.<br><br></li>
    <li><a href="http://arxiv.org/abs/1606.03966"> A Multiworld Testing Decision Service</a><br> with a number of wonderful Microsoft colleagues. Link to service <a href="https://ds.microsoft.com/">here</a>. More details <a href="https://www.microsoft.com/en-us/research/project/multi-world-testing-mwt/">here</a>.<br><br></li>
</ul>
<font face="helvetica"><font size=+2>Journal Publications</font>
  <ul>
        <li><a href="https://arxiv.org/abs/1708.01799"> Practical Evaluation and Optimization of Contextual Bandit Algorithms </a><br> with Alberto Bietti and John Langford<br>To appear in Journal of Machine Learning Research.<br><br></li>
        <li><a href="https://arxiv.org/pdf/1908.00261">  On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift  </a><br> with Sham Kakade, Jason Lee and Gaurav Mahajan<br> In Journal of Machine Learning Research, Vol. 22, 2020.<br><br></li>
        <li> <a href="http://www.jmlr.org/papers/volume20/17-681/17-681.pdf">Active Learning for Cost-Sensitive Classification</a><br> with Akshay Krishnamurthy, T.-K. Huang, Hal Daum&eacute; and John Langford<br>In Journal of Machine Learning Research, Vol. 20, 2019.<br><br></li>
    <li><a href="http://arxiv.org/pdf/1310.7991v1">Learning Sparsely Used Overcomplete Dictionaries via Alternating Minimization</a><br>with Anima Anandkumar, Prateek Jain, Praneeth Netrapalli and Rashish Tandon<br>to appear in SIAM Journal of Optimization.<br><br></li>
    <li><a href="http://arxiv.org/pdf/1309.1952v1">Exact Recovery of Sparsely Used Overcomplete Dictionaries</a><br>with Anima Anandkumar and Praneeth Netrapalli<br>In IEEE Transactions on Information Theory, Vol. 63, Issue 1, 2017.<br><br></li>
    <li><a href="http://arxiv.org/abs/1110.4198">A Reliable Effective Terascale Linear Learning System</a><br> with Olivier Chappelle, Miroslav Dudik and John Langford<br>In Journal of Machine Learning Research, Vol. 15, 2014. <br><br></li>
    <li><a href="http://arxiv.org/abs/1110.2529">The Generalization Ability of Online Algorithms for Dependent Data</a><br>with John Duchi<br>In IEEE Transactions on Information Theory, Vol. 59, Issue 1, 2013.<br><br></li>
    <li><a href="http://arxiv.org/abs/1107.1744">Stochastic convex optimization with bandit feedback</a><br> with Dean Foster, Daniel Hsu, Sham Kakade and Alexander Rakhlin<br>In SIAM Journal on Optimization, Vol. 23, Issue 1, 2013.<br><br></li>
    <li><a href="DuchiAgJoJo12.pdf">Ergodic Mirror Descent</a><br>with John Duchi, Mikael Johansson and Mike Jordan<br>In SIAM Journal on Optimization, Vol. 22, Issue 4, 2012.<br><br></li>
    <li><a href="http://arxiv.org/abs/1104.4824">Fast global convergence of gradient methods for high-dimensional statistical recovery</a><br>with Sahand Negahban and Martin Wainwright<br>In The Annals of Statistics, Vol. 40, Number 5, 2012. <br><br></li>
    <li><a href="http://arxiv.org/abs/1102.4807">Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions</a>&nbsp;&nbsp;<a href="AOS1000.pdf">(Annals formatted version)</a><br>with Sahand Negahban and Martin Wainwright<br>In The Annals of Statistics, Vol. 40, Number 2, July 2012.<br><br></li>
    <li><a href="CameraReady_IEEE.pdf">Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization</a><br>with Peter Bartlett, Pradeep Ravikumar and Martin Wainwright<br>In IEEE Transcations on Information Theory, Vol 58, Issue 5, May 2012.<br><br></li>
    <li><a href="dist_notes_ieee.pdf">Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling</a><br>with John Duchi and Martin Wainwright<br>In IEEE Transactions on Automatic Control, Vol. 57, Issue 3, 2012.<br><br></li>
<li><a href="ravikumar10b.pdf">Message-passing for graph structured linear programs: Proximal projections, convergence and rounding schemes</a><br> with Pradeep Ravikumar and Martin Wainwright<br> In Journal Of Machine Learning Research, Vol. 11, 2010.<br><br></li>
</ul>
<font face="helvetica"><font size=+2>Conference Publications</font>
  <ul>
    <li><a href="https://arxiv.org/abs/2106.06926">Bellman-consistent Pessimism for Offline Reinforcement Learning"</a><br>with Tengyang Xie, Ching-An Cheng, Nan Jiang and Paul Mineiro.<br>In NeurIPS 2021<br><br></li>
    <li><a href="https://arxiv.org/abs/2103.11559">Provably Correct Optimization and Exploration with Non-linear Policies</a><br>with Fei Feng, Lin Yang and Wotao Yin.<br>In ICML 2021<br><br></li>
    <li><a href="https://arxiv.org/abs/2103.12923">Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation</a><br> with Andrea Zanette and Ching-An Cheng.<br>In COLT 2021<br><br></li>
    <li><a href="https://arxiv.org/abs/2103.10620"> Towards a Dimension-Free Understanding of Adaptive Linear Control </a><br> with Juan Perdomo, Max Simchowitz and Peter Bartlett.<br>In COLT 2021<br><br></li>
  <li><a href="https://arxiv.org/abs/2007.08459">PC-PG: Policy Cover Directed Exploration for Provable Policy Gradient Learning </a> <br>with Mikael Henaff, Sham Kakade and Wen Sun.<br>In NeurIPS 2020<br><br></li>
  <li><a href="https://arxiv.org/abs/2007.08202"> Provably Good Batch Reinforcement Learning Without Great Exploration </a><br>with Yao Liu, Adith Swaminathan and Emma Brunskill.<br>In NeurIPS 2020<br><br></li>
  <li><a href="https://arxiv.org/abs/2007.00795"> Policy Improvement from Multiple Experts <br> with Ching-An Cheng and Andrey Kolobov.<br>In NeurIPS 2020<br><br></li>
  <li><a href="https://arxiv.org/abs/2006.12136">  Safe Reinforcement Learning via Curriculum Induction <br> with Matteo Turchetta, Andrey Kolobov, Shital Shah and Andreas Krause.<br>In NeurIPS 2020<br><br> </li>
  <li><a href="https://arxiv.org/abs/2006.10814">FLAMBE: Structural Complexity and Representation Learning of Low Rank MDPs </a><br>with Sham Kakade, Akshay Krishnamurthy and Wen Sun.<br>In NeurIPS 2020, oral presentation <br><br></li>
  <li><a href="https://arxiv.org/abs/2003.01922"> Taking a hint: How to leverage loss predictors in contextual bandits? <br> with Chen-Yu Wei and Haipeng Luo.<br>In COLT 2020<br><br></li>
  <li><a href="https://arxiv.org/pdf/1908.00261">  On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift  </a><br> with Sham Kakade, Jason Lee and Gaurav Mahajan<br> In COLT 2020<br><br></li>
      <li><a href="https://arxiv.org/pdf/1906.03804"> On the Optimality of Sparse Model-Based Planning for Markov Decision Processes </a><br>with Sham Kakade and Lin Yang.<br> In COLT 2020<br><br></li>
      <li><a href="https://arxiv.org/abs/1906.03671"> Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds </a><br>with Jordan Ash, Chicheng Zhang, Akshay Krishnamurthy and John Langford.<br>In ICLR 2020<br><br></li>
      <li><a href="https://arxiv.org/pdf/1905.05179">Metareasoning in Modular Software Systems: On-the-Fly Configuration using Reinforcement Learning with Rich Contextual Representations</a> <br> with Aditya Modi, Debadeepta Dey, Adith Swaminathan, Besmira Nushi, Sean Andrist and Eric Horvitz.<br>In AAAI 2020<br><br></li>
    <li><a href="https://arxiv.org/pdf/1904.08473"> Off-Policy Policy Gradient with State Distribution Correction </a><br> with Yao Liu, Adith Swaminathan and Emma Brunskill.<br>In UAI 2019<br><br></li>
  <li><a href="https://arxiv.org/pdf/1905.12843">Fair Regression: Quantitative Definitions and Reduction-based Algorithms</a><br> with Steven Wu and Miro Dudik.<br>In ICML 2019<br><br></li>
  <li><a href="https://arxiv.org/abs/1901.09018">Provably efficient RL with Rich Observations via Latent State Decoding</a><br> with Simon Du, Akshay Krishnamurthy, Nan Jiang, Miro Dudik and John Langford.<br>In ICML 2019<br><br></li>
    <li><a href="https://arxiv.org/abs/1901.00301">Warm-starting Contextual Bandits: Robustly Combining Supervised and Bandit Feedback</a><br> with Chicheng Zhang, Hal Daum&eacute;, John Langford and Sahand Negahban.<br>In ICML 2019<br><br></li>
    <li><a href="https://arxiv.org/abs/1811.08540">Model-based RL in Contextual Decision Processes: PAC bounds and Exponential Improvements over Model-free Approaches </a><br> with Wen Sun, Nan Jiang, Akshay Krishnamurthy and John Langford.<br>In COLT 2019<br><br></li>
        <li><a href="https://arxiv.org/abs/1803.00606"> On Polynomial Time PAC Reinforcement Learning with Rich Observations </a><br> with Christoph Dann, Nan Jiang, Akshay Krishnamurthy, John Langford and Rob Schapire.<br>In NeurIPS 2018<br><br></li>
        <li><a href="https://arxiv.org/abs/1803.02453"> A Reductions Approach to Fair Classification </a><br> with Alina Beygelzimer, Miro Dudik, John Langford and Hanna Wallach.<br>In ICML 2018<br><br></li>
    <li><a href="https://arxiv.org/abs/1803.01088"> Practical Contextual Bandits with Regression Oracles </a><br> with Dylan Foster, Haipeng Luo, Miro Dudik and Rob Schapire.<br>In ICML 2018<br><br></li>
	<li><a href="https://arxiv.org/abs/1803.00590"> Hierarchical Imitation and Reinfocement Learning </a><br> with Hoang Le, Nan Jiang, Miro Dudik, Yisong Yue and Hal Daum&eacute;.<br>In ICML 2018<br><br></li>
	    <li><a href="https://arxiv.org/abs/1708.01799"> Efficient Contextual Bandits in Non-stationary Worlds </a><br> with Haipeng Luo, Chen-Yu Wei and John Langford.<br>In COLT 2018<br><br></li>
        <li><a href="http://arxiv.org/abs/1605.04812"> Off-policy evaluation for slate recommendation</a><br>with Adith Swaminathan, Akshay Krishnamurthy, Miro Dudik, John Langford, Damien Jose and Imed Zitouni<br>In NIPS 2017, oral presentation<br><br></li>
        <li><a href="https://arxiv.org/abs/1612.06246">Corralling a Band of Bandit Algorithms</a><br> with Haipeng Luo, Behnam Neyshabur and Rob Schapire<br>In COLT 2017<br><br></li>
        <li><a href="https://arxiv.org/abs/1703.01014">Active Learning for Cost-Sensitive Classification</a><br> with Akshay Krishnamurthy, T-K Huang, Hal Daum&eacute; III and John Langford<br>In ICML 2017<br><br></li>
        <li><a href="https://arxiv.org/abs/1610.09512">  Contextual Decision Processes with Low Bellman Rank are PAC-Learnable</a><br> with Nan Jiang, Akshay Krishnamurthy, John Langford and Rob Schapire<br>In ICML 2017<br><br></li>
         <li><a href="https://arxiv.org/abs/1612.01205">  Optimal and Adaptive Off-policy Evaluation in Contextual Bandits</a><br> with Yu-Xiang Wang and Miro Dudik<br>In ICML 2017<br><br></li>
         <li><a href="http://arxiv.org/abs/1602.02722"> Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations</a><br>with Akshay Krishnamurthy and John Langford<br>In NIPS 2016<br><br></li>
         <li><a href="http://arxiv.org/abs/1602.02202">Efficient Second Order Online Learning by Sketching</a><br>with Haipeng Luo, Nicolo Cesa-Bianchi and John Langford<br>In NIPS 2016<br><br></li>
         <li><a href="http://arxiv.org/abs/1502.05890">Efficient Contextual Semi-Bandit Learning</a><br>with Akshay Krishnamurthy and Miro Dudik<br>In NIPS 2016<br><br></li>
         <li><a href="http://arxiv.org/abs/1507.00407">Fast Convergence of Regularized Learning in Games</a>    (<b>Best paper award</b>)<br>with Vasilis Syrgkanis, Haipeng Luo and Rob Schapire<br>In NIPS 2015<br><br></li>
         <li><a href="http://arxiv.org/abs/1506.08669">Efficient and Parsimonious Agnostic Active Learning</a><br>with T-K Huang, Daniel Hsu, John Langford and Rob Schapire<br>In NIPS 2015<br><br></li>
         <li><a href="http://arxiv.org/abs/1502.02206">Learning to Search Better Than Your Teacher</a><br>with Kai-Wei Chang, Akshay Krishnamurthy, Hal Daum&eacute; and John Langford<br>In ICML 2015<br><br></li>
         <li><a href="http://arxiv.org/abs/1410.0723">A Lower Bound for the Optimization of Finite Sums</a><br>with L&eacute;on Bottou<br>In ICML 2015<br><br></li>
         <li><a href="http://arxiv.org/pdf/1410.0440">Scalable Nonlinear Learning with Adaptive Polynomial Expansions</a><br>with Alina Beygelzimer, Daniel Hsu, John Langford and Matus Telgarsky<br>In NIPS 2014<br><br></li>
         <li><a href="dict-learning-colt.pdf">Learning sparsely used overcomplete dictionaries</a><br>with Anima Anandkumar, Prateek Jain, Praneeth Netrapalli and Rashish Tandon<br>In COLT 2014<br><br></li>
         <li><a href="manager.pdf">Robust Multi-Objective Learning with Mentor Feedback</a><br>with Ashwinkumar BV, Miro Dudik, Rob Schapire and Alex Slivkins<br>In COLT 2014<br><br></li>
         <li><a href="http://arxiv.org/abs/1402.0555">Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits</a><br>with Daniel Hsu, Satyen Kale, John Langford, Lihong Li and Rob Schapire<br>In ICML 2014<br><br></li>
         <li><a href="http://arxiv.org/pdf/1310.1949v2">Least Squares Revisited: Scalable Approaches for Multi-class Prediction</a><br>with Sham Kakade, Nikos Karampatziakis, Le Song and Greg Valiant<br>In ICML 2014<br><br></li>
         <li><a href="multiclass.pdf">Selective sampling algorithms for cost-sensitive multiclass prediction</a> (long version with proofs)<br>In ICML 2013<br><br></li>
         <li><a href="http://arxiv.org/abs/1207.4421">Stochastic optimization and sparse statistical recovery: An optimal algorithm for high dimensions</a>  (<a href="http://arxiv.org/abs/1207.4421">Long version</a>)<br>with Sahand Negahban and Martin Wainwright<br>In NIPS 2012<br><br></li>
         <li><a href="http://arxiv.org/abs/1202.1334">Contextual Bandit Learning with Predictable Rewards</a><br> with Miroslav Dudik, Satyen Kale, John Langford and Robert Schapire<br> In AISTATS 2012<br><br></li>
         <li><a href="http://arxiv.org/abs/1107.1744">Stochastic convex optimization with bandit feedback</a><br> with Dean Foster, Daniel Hsu, Sham Kakade and Alexander Rakhlin<br>In NIPS 2011<br><br></li>
         <li><a href="http://arxiv.org/abs/1104.5525">Distributed Delayed Stochastic Optimization</a>  (<a href="http://arxiv.org/abs/1104.5525">Long version</a>)<br>with John Duchi<br>In NIPS 2011<br><br></li>
         <li><a href="http://arxiv.org/abs/1105.4681">Ergodic Subgradient Descent</a><br>with John Duchi, Mikael Johansson and Mike Jordan<br>In Allerton 2011 <br><br></li>
         <li><a href="http://eecs.berkeley.edu/~arostami/papers/corrupted_features.pdf">Learning with Missing Features</a><br>with Afshin Rostamizadeh and Peter Bartlett<br>In UAI 2011 <br><br></li>
         <li><a href="http://jmlr.csail.mit.edu/proceedings/papers/v19/agarwal11a/agarwal11a.pdf">Oracle inequalities for computationally budgeted model selection</a>  (<a href="http://arxiv.org/abs/1208.0129">Long version</a>)<br>with John Duchi, Peter Bartlett and Clement Levrard<br>In COLT 2011 <br><br></li>
         <li><a href="http://arxiv.org/abs/1102.4807">Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions</a><br>with Sahand Negahban and Martin Wainwright<br>In ICML 2011 <br><br></li>
        <li><a href="distopt_nips.pdf">DIStributed Dual Averaging In Networks</a><br>with John Duchi and Martin Wainwright<br>In NIPS 2010.<br><br></li>
	<li><a href="sparseopt_nips.pdf">Convergence rates of gradient methods for high-dimensional statistical recovery</a><br>with Sahand Negahban and Martin Wainwright<br>In NIPS 2010, oral presentation<br><br></li>
        <li><a href="bandits-colt.pdf">Optimal Algorithms for Online Convex Optimization with Multi-Point Bandit Feedback</a> (longer version with additional proofs)<br>with Ofer Dekel and Lin Xiao<br>In COLT 2010.<br><br></li>
	<li><a href="300_paper.pdf">Optimal Allocation Strategies for the Dark Pool Problem</a><br> with Peter Bartlett and Max Dama<br>In AISTATS 2010.<br><br></li>
	<li><a href="1005_paper.pdf">Information-theoretic lower bounds on the oracle complexity of convex optimization</a><br>with Peter Bartlett, Pradeep Ravikumar and Martin Wainwright<br>In NIPS 2009.<br><br></li>
	<li><a href="http://arxiv.org/abs/0903.5328">A Stochastic View of Optimal Regret through Minimax Duality</a><br>with Jake Abernethy, Alexander Rakhlin and Peter Bartlett<br>arXiv preprint, short version appeared in COLT 2009.<br><br></li>	
	<li><a href="I_prox08_tech.pdf">Message-passing for graph structured linear programs: Proximal projections, convergence and rounding schemes</a><br> with Pradeep Ravikumar and Martin Wainwright<br>In ICML 2008.<br><br></li>
<li><a href="http://books.nips.cc/papers/files/nips20/NIPS2007_0780.pdf">An Analysis of Inference with the Universum</a><br>with Fabian Sinze, Olivier Chapelle and Bernhard Sch&ouml;lkopf<br>In <a href="http://www.nips.cc/">NIPS 2007</a><br><br></li>
<li><a href="http://www.cse.iitb.ac.in/~soumen/doc/netrank">Learning Random Walks to Rank Nodes in Graphs</a><br>with Soumen Chakrabarti<br>In <a href="http://oregonstate.edu/conferences/icml2007/">ICML 2007</a><br><br></li>
<li><a href="http://www.cse.iitb.ac.in/~soumen/doc/netrank">Learning Parameters in Entity-relationship Graphs from Ranking Preferences<br></a>with Soumen Chakrabarti<br>
In <a href="http://www.ecmlpkdd2006.org/"> ECML/PKDD 2006</a><br><br></li>
<li><a href="http://www.cse.iitb.ac.in/~soumen/doc/netrank">Learning to Rank Networked Entities</a><br>with Soumen Chakrabarti and Sunny Aggarwal<br>
In <a href="http://www.kdd2006.com/"> SIGKDD 2006</a><br><br></li>
</ul>
<tr><td width=600 align="left"><br><font face="helvetica"><font size=+2>Teaching</font>
<a name="teaching"></a>
<hr width=990 align="center">
<br>
<a href="https://courses.cs.washington.edu/courses/cse599m/19sp/">CSE 599: Reinforcement Learning and Bandits</a>, taught at University of Washington in Spring 2019 with <a href="https://homes.cs.washington.edu/~sham/">Sham Kakade</a>.
<br>
<a href="http://alekhagarwal.net/bandits_and_rl/">Bandits and Reinforcement Learning</a>, taught at Columbia University in Fall 2017 with <a href="https://www.microsoft.com/en-us/research/people/slivkins/">Alex Slivkins</a>.
<br><br>
</td></tr>
<tr><td width=600 align="left"><br><font face="helvetica"><font size=+2>Professional Activities</font>
<a name="professional"></a>
<hr width=990 align="center">
<br>
Fundraising Chair for <a href="http://aistats.org/">AISTATS 2016.</a>
<br>
Co-organized NIPS 2015 workshop on <a href="http://opt-ml.org/">Optimization for Machine Learning</a>.
<br>
Co-organized NIPS 2014 workshop on <a href="http://opt-ml.org/">Optimization for Machine Learning</a>.
<br>
Co-organized NIPS 2013 workshop on <a href="http://opt-ml.org/">Optimization for Machine Learning</a>.
<br>
Co-organized NIPS 2013 workshop on <a href="http://opt.kyb.tuebingen.mpg.de/index.html">Optimization for Machine Learning</a>.
<br>
Co-organized NIPS 2012 workshop on <a href="http://opt.kyb.tuebingen.mpg.de/index.html">Optimization for Machine Learning</a>.
<br>
Co-organized NIPS 2011 workshop on <a href="https://sites.google.come/site/costnips">Computational Trade-offs in Statistical Learning</a>.
<br>
Co-organized NIPS 2010 workshop <a href="http://lccc.eecs.berkeley.edu">Learning on Cores, Clusters and Clouds</a>.
<br>
<b>Senior Area Chair: </b> NeurIPS 2019, NeurIPS 2020.
<br>
<b>Area chair or equivalent:</b> ICML 2013-2020, NeurIPS 2013-2018, COLT 2013-2020, AISTATS 2013, NeurIPS 2013.
<br>
<b>Journal Reviewing:</b> JMLR, Annals of Statistics, IEEE Transcations on Automatic Control, IEEE Transcations on Info Theory, SIAM Journal on Optimization, Machine Learning. <br><br>
</body>

<!--footer code begin-->
<div style="font-family: Verdana; width:1002px; height:25px; padding-top:5px; font-size:60%;">
<a href="/c/1060" style="border-right:1px solid #bbb; padding:0em 1em 0em 0em;">Contact</a>
<a href="/c/1061" style="border-right:1px solid #bbb; padding:0em 1em 0em 1em;">Terms</a>
<a href="/c/1062" style="border-right:1px solid #bbb; padding:0em 1em 0em 1em;">Trademarks</a>
<a href="/c/1063" style="border-right:1px solid #bbb; padding:0em 1em 0em 1em;">Privacy and Cookies</a>
<a href="/c/1065" style="padding:0em 1em 0em 1em;">Code of Conduct</a>
<span style="margin-right:100px;">&copy;<span id="copyrightYear"></span> Microsoft Corporation. All rights reserved.</span><a href="/c/1064"><img style="margin-right:10px;" alt="Microsoft" src="/a/i/c/logo_ms.png" border="0"></a>
<script language="javascript" src="/a/year.js" type="text/javascript"></script>
</div>
<!--footer code end-->
</html>

