<!DOCTYPE html>
<html>
<head>
    <title>Bandits &amp; RL</title>
    <script src="tabcontent.js" type="text/javascript"></script>
    <link href="template2/tabcontent.css" rel="stylesheet" type="text/css" />
	<style type="text/css">
	.auto-style1 {
		margin-top: 47;
	}
	.auto-style2 {
	text-align: center;
}

.content {
  color: #222222;
  font: 11pt Helvetica, Arial, sans-serif;
  text-decoration : none;
  padding-top: 10pt;
  padding-bottom: 10pt;
  padding-left: 10pt;
  padding-right: 15pt;
  text-align: left;
}

	.auto-style3 {
	color: #FF0000;
}

	.auto-style4 {
	background-color: #FFFF00;
}

	</style>
</head>
<body style="background:#F6F9FC; font-family:Arial;">
<h3 class="auto-style2">Bandits and Reinforcement Learning (Fall 2017)</h3>
<div style="width: 677px; padding: 0px 0 40px; margin-left: auto; margin-right: auto; margin-bottom: 0;" class="auto-style1">
        <ul class="tabs" data-persist="true">
            <li visible="true"><a href="#view1">Course Info</a></li>
            <li><a href="#view2">Lectures</a></li>
			<li><a href="#view3">Project</a></li>
        </ul>
		<div class="tabcontents">

            <div id="view1">
            <div class="content">
                <p><strong>Roster/waitlist situation (Sept 13): </strong>we have 
				an almost full roster and a large waitlist .&nbsp; </p>
				<ul>
					<li>Please&nbsp;add yourself to the waitlist if you are 
					interested. We give preference to PhD students. We will 
					start adding Masters students to the roster in a day or two.</li>
					<li><strong>If you decide to drop the 
					course</strong>, we kindly ask you to enter it into SSOL <em>
					soon</em>.</li>
				</ul>
				<p><strong>Course number</strong>: COMS E6998.001, Columbia 
				University<br><strong>Instructors</strong>:
				<a href="http://alekhagarwal.net/">Alekh Agarwal</a> and
				<a href="https://www.microsoft.com/en-us/research/people/slivkins/">
				Alex Slivkins</a> (Microsoft Research NYC)<br>
				<span class="section-days"><strong>Schedule</strong>: Wednesdays</span>
				4:10-6:40pm<br><span class="class-end-time">
				<span class="class-building"><strong>Location:</strong>
				404 International Affairs Building<strong>.<br>Office Hours</strong>: 
				after each class, and online (see below). <br><strong>Q&amp;A and 
				announcements:</strong> 
				we use a
				<a href="http://piazza.com/columbia/fall2017/comse6998001/home">
				Piazza site</a> (<a href="http://piazza.com/columbia/fall2017/comse6998001"><span class="auto-style4">sign 
				up</span></a>)<br> <strong>
				Contact:</strong> 
				<a href="mailto:bandits-fa17-instr@microsoft.com">
				bandits-fa17-instr@microsoft.com</a>.</span></span></p>
				<p><strong>Online office hours</strong></p>
				<p><span class="class-end-time">
				<span class="class-building">[Alekh] Fridays&nbsp;&nbsp;&nbsp; 
				10:30-11:30am by appointment.<br>[Alex]&nbsp;&nbsp; Mondays 
				11am-noon by appointment.</span></span></p>
				<p>Technical details to follow. Please send us an email a day in 
				advance. </p>
				<p>Please consider asking via Piazza first, for everyone to see 
				a reply. </p>
				<p><span class="class-end-time"><strong>Course 
				description</strong> </span></p>
				<p><span class="class-end-time">This course covers several 
				foundational topics in sequential decision making. We start with 
				multi-armed bandits, and proceed to contextual bandits, a 
				generalization motivated by several real-world applications. We 
				cover the main algorithms, performance guarantees and lower 
				bounds, along with applications in the industry. We also discuss 
				“offline off-policy evaluation”: performance assessment 
				techniques for real-world online systems. The remainder of the 
				course concerns the more general problem of reinforcement 
				learning, from small-state MDPs to recent extensions that handle 
				large state spaces. </span></p>
				<p><span class="class-end-time"><i>Throughout, the course will primarily focus on the
				theoretical foundations, with some discussion of the practical aspects.</i> </span></p>
				<p><strong>Prerequesites </strong></p>
				<p>Exposure to algorithms and proofs at the level of an 
				undergraduate algorithms course (CSOR 4231). Graduate ML course 
				(COMS 4771) or current enrollment therein. If you do not meet 
				these, please email the instructors. </p>
				<p><strong>Coursework and assessment</strong></p>
				<p>A final project, 
				letter grade. The project will involve a mix of reading, coding, 
				and/or original research on a course-related topic of your 
				choice.</p>
				<p><strong>Tentative topics</strong></p>
				<ul>
					<li>Fundamental algorithms for multi-armed bandits:<ul>
						<li>IID rewards: UCB1, Successive Elimination</li>
						<li>adversarial rewards: multiplicative weights, 
						follow-the-leader, EXP3/EXP4</li>
						<li>Bayesian/IID rewards: Thompson Sampling</li>
					</ul>
					</li>
					<li>Advanced topics in multi-armed bandits (*)<ul>
						<li>Lower 
						bounds</li>
						<li>structured action spaces (e.g., Lipschitz, 
						linear, combinatorial)</li>
						<li>global constraints ("bandits 
						with knapsacks")</li>
						<li>connections to game theory and 
						mechanism design</li>
					</ul>
					(*) some but not all of these topics 
						will be covered</li>
					<li>Contextual Bandits (CB)<ul>
						<li>CB 
						with classification oracle: Epsilon-greedy, "Taming the 
						Monster"</li>
						<li>CB with linear rewards: LinUCB</li>
						<li>Off-Policy 
						Evaluation</li>
					</ul>
					</li>
					<li>Reinforcement Learning<ul>
						<li>MDPs, Value 
						functions, Value Iteration, Q-Learning</li>
						<li>R-MAX, lower 
						bounds</li>
						<li>Bellman Rank and resulting algorithm<br></li>
					</ul>
					</li>
				</ul>
				<p><strong>Resources</strong></p>
				<p>Some of the lectures will be based on chapters from
				<a href="http://slivkins.com/work/MAB-book.pdf">this book 
				draft</a>. For the rest, reading and/or lecture notes will be 
				provided. </p>
				<p>Some resources on course-related topics are listed below:</p>
				<ul>
					<li>
					<a href="http://research.microsoft.com/en-us/um/people/sebubeck/SurveyBCB12.pdf">
					A survey on (non-Bayesian) bandits</a><span>&nbsp;</span>by 
					Sebastien Bubeck and Nicolo Cesa-Bianchi (2012).</li>
					<li>
					<a href="http://www.cambridge.org/us/catalogue/catalogue.asp?isbn=0521841089">
					Prediction, Learning, and Games</a>: a book by Nicolo 
					Cesa-Bianchi and Gabor Lugosi (2006).</li>
					<li>(Brief) lectures on bandit theory by Sebastien Bubeck:<span>&nbsp;</span><a href="https://blogs.princeton.edu/imabandit/2016/05/11/bandit-theory-part-i/">part 
					1</a><span>&nbsp;</span>and<span>&nbsp;</span><a href="https://blogs.princeton.edu/imabandit/2016/05/13/bandit-theory-part-ii/">part 
					2</a><span>&nbsp;</span>(Spring'16).</li>
					<li>A<span>&nbsp;</span><a href="http://www.math.ucsd.edu/~fan/complex/ch2.pdf">survey 
					on concentration inequalities</a><span>&nbsp;</span>by Fan Chung 
					and Linyuan Lu (2010)<br>Another<span>&nbsp;</span><a href="http://www.stats.ox.ac.uk/__data/assets/pdf_file/0017/4139/montpelierconc.pdf">survey 
					on concentration inequalities</a><span>&nbsp;</span>by Colin 
					McDiarmid (1998).</li>
				</ul>
				<p><b>Concurrent and upcoming courses at Columbia</b></p>
				<ul>
					<li>Daniel Russo is teaching a 
					<a href="https://djrusso.github.io/RLCourse/index">course on dynamic optimization and reinforcement learning</a> 
						in Fall'17 (in the Business School). </li>
					<li>Shipra Agrawal will be teaching a 
						course on reinforcement learning in Spring'18 (in the 
						IEOR department). </li>
				</ul>
				<p>Our course focuses more heavily on contextual bandits and off-policy evaluation than either of these, and is complimentary to these other offerings. </p>
				<p><strong>Recent courses with closely related content</strong></p>
				<ul>
				        <li>Shipra Agrawal,
					<a href="http://bandits.wikischolars.columbia.edu/">Learning 
					and Optimization for Sequential Decision Making</a>, 
					Columbia University, IEOR dept, Spring 2016.</li>
					<li>Alex Slivkins,
					<a href="http://www.cs.umd.edu/~slivkins/CMSC858G-fall16/">
					Bandits, Experts, and Games</a>, University of Maryland, 
					Computer Science Department, Fall 2016.</li>
					<li>Csaba Szepesvari and Tor Lattimore,
					<a href="http://banditalgs.com/">Bandit Algorithms</a>, 
					University of Alberta (Fall 2016) and Indiana 
					University(Spring 2017).</li>
				</ul>
				<p>&nbsp;</p>
            </div>
			</div>
            <div id="view2">
            	<div class="content">
					<strong>Lecture #1 (Sep 6 | Alex)</strong>: <span>&nbsp;Class 
					organization and intro to the problem space (</span><a href="intro.pdf">slides</a><span>).<br>
					<br>[Brief and very basic] recap of probability and concentration inequalities (<a href="intro-probab.pdf">slides)</a>.<br>
					<span class="auto-style3">Understanding these slides is 
					necessary for the rest of the course!</span><br>
					<br><strong>Lecture #2 (Sep 13 | Alex)</strong>: Multi-armed 
					bandits with IID rewards.&nbsp;<br>Reference: chapter 2 of
					<a href="http://slivkins.com/work/MAB-book.pdf">Alex's book 
					draft</a>.<br><br><strong>Lecture #3 (Sep 20 | Alex):</strong>&nbsp;Lower 
					bounds on regret<br>Reference: chapter 3 of
					<a href="http://slivkins.com/work/MAB-book.pdf">Alex's book 
					draft</a>.<br><br><strong>Lecture #4 (Sep 27 | Alex)</strong> 
					The best-expert problem (full feedback &amp; adversarial 
					rewards)<br>Reference: chapter 6 of
					<a href="http://slivkins.com/work/MAB-book.pdf">Alex's book 
					draft</a>.<br><br><strong>Lecture #5 (Oct 4 | Alex)</strong> 
					Adversarial bandits<br>Reference: chapter 7 of
					<a href="http://slivkins.com/work/MAB-book.pdf">Alex's book 
					draft</a>.<br><br><strong>Lecture #6 (Oct 11 | Alekh)</strong> 
					TBD<br><br><strong>Lecture #7 ( Oct 18 | Alekh)</strong> TBD<br>
					<br><strong>Lecture #8 ( Oct 25 | Alekh)</strong> TBD <br>
					<br><strong>Lecture #9 ( Nov 1 | Alekh)</strong> TBD<br><br>
					<strong>Lecture #10 (Nov 8 | Alekh)</strong> TBD<br><br>
					<strong>Lecture #11 (Nov 15 | Alekh)</strong> TBD<br><br>
					<strong>Lecture #12 (Nov 29 | Alex)</strong> Bandits &amp; 
					incentives (tent.)<br><br><strong>Lecture #13 (Dec 6 | Alex)</strong> 
					project presentations<br></span></div>
			</div>
			
			<div id="view3">
            <div class="content">
                
                <p>coming ... </p>
			</div>
			</div>

        </div>
    </div>
</body>
</html>
