<!DOCTYPE html>
<html>
<head>
    <title>Bandits &amp; RL</title>
    <script src="tabcontent.js" type="text/javascript"></script>
    <link href="template2/tabcontent.css" rel="stylesheet" type="text/css" />
	<style type="text/css">
	.auto-style1 {
		margin-top: 47;
	}
	.auto-style2 {
	text-align: center;
}

.content {
  color: #222222;
  font: 11pt Helvetica, Arial, sans-serif;
  text-decoration : none;
  padding-top: 10pt;
  padding-bottom: 10pt;
  padding-left: 10pt;
  padding-right: 15pt;
  text-align: left;
}

	.auto-style3 {
	color: #FF0000;
}

	.auto-style4 {
	background-color: #FFFFFF;
}

	.auto-style5 {
	color: #008000;
}
.auto-style6 {
	color: #800000;
}

	</style>
</head>
<body style="background:#F6F9FC; font-family:Arial;">
<h3 class="auto-style2">Bandits and Reinforcement Learning (Fall 2017)</h3>
<div style="width: 677px; padding: 0px 0 40px; margin-left: auto; margin-right: auto; margin-bottom: 0;" class="auto-style1">
        <ul class="tabs" data-persist="true">
            <li visible="true"><a href="#view1">Course Info</a></li>
            <li><a href="#view2">Lectures</a></li>
			<li><a href="#view3">Project</a></li>
			<li><a href="#view4">Homeworks</a></li>
        </ul>
		<div class="tabcontents">

            <div id="view1">
            <div class="content">
				<p><strong>Course number</strong>: COMS E6998.001, Columbia 
				University<br><strong>Instructors</strong>:
				<a href="http://alekhagarwal.net/">Alekh Agarwal</a> and
				<a href="https://www.microsoft.com/en-us/research/people/slivkins/">
				Alex Slivkins</a> (Microsoft Research NYC)<br>
				<span class="section-days"><strong>Schedule</strong>: Wednesdays</span>
				4:10-6:40pm<br><span class="class-end-time">
				<span class="class-building"><strong>Location:</strong>
				404 International Affairs Building<strong>.<br>Office Hours</strong>: 
				after each class, and online (see below). <br><strong>Q&amp;A and 
				announcements:</strong> 
				we use a
				<a href="http://piazza.com/columbia/fall2017/comse6998001/home">
				Piazza site</a> <span class="auto-style4">(</span><a href="http://piazza.com/columbia/fall2017/comse6998001"><span class="auto-style4">sign 
				up</span></a>). We are <strong>not</strong> using Courseworks.<br> <strong>
				Contact:</strong> 
				<a href="mailto:bandits-fa17-instr@microsoft.com">
				bandits-fa17-instr@microsoft.com</a>.</span></span></p>
				<p><strong>Online office hours</strong></p>
				<p><span class="class-end-time">
				<span class="class-building">[Alekh] Fridays&nbsp;&nbsp;&nbsp; 
				10:30-11:30am by appointment.<br>[Alex]&nbsp;&nbsp; Mondays 
				11am-noon by appointment (over Skype).</span></span></p>
				<p>Please send us an email [at least] the 
				night before.</p>
				<p>Please consider asking via Piazza first, for everyone to see 
				a reply. </p>
				<p><span class="class-end-time"><strong>Course 
				description</strong> </span></p>
				<p><span class="class-end-time">This course covers several 
				foundational topics in sequential decision making. We start with 
				multi-armed bandits, and proceed to contextual bandits, a 
				generalization motivated by several real-world applications. We 
				cover the main algorithms, performance guarantees and lower 
				bounds, along with applications in the industry. We also discuss 
				“offline off-policy evaluation”: performance assessment 
				techniques for real-world online systems. The remainder of the 
				course concerns the more general problem of reinforcement 
				learning, from small-state MDPs to recent extensions that handle 
				large state spaces. </span></p>
				<p><span class="class-end-time"><i>Throughout, the course will primarily focus on the
				theoretical foundations, with some discussion of the practical aspects.</i> </span></p>
				<p><strong>Prerequesites </strong></p>
				<p>Exposure to algorithms and proofs at the level of an 
				undergraduate algorithms course (CSOR 4231). Graduate ML course 
				(COMS 4771) or current enrollment therein. If you do not meet 
				these, please email the instructors. </p>
				<p><strong>Coursework and assessment</strong></p>
				<p>A final project, 
				letter grade. The project will involve a mix of reading, coding, 
				and/or original research on a course-related topic of your 
				choice.</p>
				<p><strong>Tentative topics</strong></p>
				<ul>
					<li>Fundamental algorithms for multi-armed bandits:<ul>
						<li>IID rewards: UCB1, Successive Elimination</li>
						<li>adversarial rewards: multiplicative weights, 
						follow-the-leader, EXP3/EXP4</li>
						<li>Bayesian/IID rewards: Thompson Sampling</li>
					</ul>
					</li>
					<li>Advanced topics in multi-armed bandits (*)<ul>
						<li>Lower 
						bounds</li>
						<li>structured action spaces (e.g., Lipschitz, 
						linear, combinatorial)</li>
						<li>global constraints ("bandits 
						with knapsacks")</li>
						<li>connections to game theory and 
						mechanism design</li>
					</ul>
					(*) some but not all of these topics 
						will be covered</li>
					<li>Contextual Bandits (CB)<ul>
						<li>CB 
						with classification oracle: Epsilon-greedy, "Taming the 
						Monster"</li>
						<li>CB with linear rewards: LinUCB</li>
						<li>Off-Policy 
						Evaluation</li>
					</ul>
					</li>
					<li>Reinforcement Learning<ul>
						<li>MDPs, Value 
						functions, Value Iteration, Q-Learning</li>
						<li>R-MAX, lower 
						bounds</li>
						<li>Bellman Rank and resulting algorithm<br></li>
					</ul>
					</li>
				</ul>
				<p><strong>Resources</strong></p>
				<p>Some of the lectures will be based on chapters from
				<a href="http://slivkins.com/work/MAB-book.pdf">this book 
				draft</a>. For the rest, reading and/or lecture notes will be 
				provided. </p>
				<p>Some resources on course-related topics are listed below:</p>
				<ul>
					<li>
					<a href="http://research.microsoft.com/en-us/um/people/sebubeck/SurveyBCB12.pdf">
					A survey on (non-Bayesian) bandits</a><span>&nbsp;</span>by 
					Sebastien Bubeck and Nicolo Cesa-Bianchi (2012).</li>
					<li>
					<a href="http://www.cambridge.org/us/catalogue/catalogue.asp?isbn=0521841089">
					Prediction, Learning, and Games</a>: a book by Nicolo 
					Cesa-Bianchi and Gabor Lugosi (2006).</li>
					<li>(Brief) lectures on bandit theory by Sebastien Bubeck:<span>&nbsp;</span><a href="https://blogs.princeton.edu/imabandit/2016/05/11/bandit-theory-part-i/">part 
					1</a><span>&nbsp;</span>and<span>&nbsp;</span><a href="https://blogs.princeton.edu/imabandit/2016/05/13/bandit-theory-part-ii/">part 
					2</a><span>&nbsp;</span>(Spring'16).</li>
					<li>A<span>&nbsp;</span><a href="http://www.math.ucsd.edu/~fan/complex/ch2.pdf">survey 
					on concentration inequalities</a><span>&nbsp;</span>by Fan Chung 
					and Linyuan Lu (2010)<br>Another<span>&nbsp;</span><a href="http://www.stats.ox.ac.uk/__data/assets/pdf_file/0017/4139/montpelierconc.pdf">survey 
					on concentration inequalities</a><span>&nbsp;</span>by Colin 
					McDiarmid (1998).</li>
				</ul>
				<p><b>Concurrent and upcoming courses at Columbia</b></p>
				<ul>
					<li>Daniel Russo is teaching a 
					<a href="https://djrusso.github.io/RLCourse/index">course on dynamic optimization and reinforcement learning</a> 
						in Fall'17 (in the Business School). </li>
					<li>Shipra Agrawal will be teaching a 
						course on reinforcement learning in Spring'18 (in the 
						IEOR department). </li>
				</ul>
				<p>Our course focuses more heavily on contextual bandits and off-policy evaluation than either of these, and is complimentary to these other offerings. </p>
				<p><strong>Recent courses with closely related content</strong></p>
				<ul>
				        <li>Shipra Agrawal,
					<a href="http://bandits.wikischolars.columbia.edu/">Learning 
					and Optimization for Sequential Decision Making</a>, 
					Columbia University, IEOR dept, Spring 2016.</li>
					<li>Alex Slivkins,
					<a href="http://www.cs.umd.edu/~slivkins/CMSC858G-fall16/">
					Bandits, Experts, and Games</a>, University of Maryland, 
					Computer Science Department, Fall 2016.</li>
					<li>Csaba Szepesvari and Tor Lattimore,
					<a href="http://banditalgs.com/">Bandit Algorithms</a>, 
					University of Alberta (Fall 2016) and Indiana 
					University(Spring 2017).</li>
				</ul>
				<p>&nbsp;</p>
            </div>
			</div>
            <div id="view2">
            	<div class="content">
					<strong>Lecture #1 (Sep 6 | Alex)</strong>: <span>&nbsp;Class 
					organization and intro to the problem space (</span><a href="intro.pdf">slides</a><span>).<br>
					<br>[Brief and very basic] recap of probability and concentration inequalities (<a href="intro-probab.pdf">slides)</a>.<br>
					<span class="auto-style3">Understanding these slides is 
					necessary for the rest of the course!</span><br>
					<br><strong>Lecture #2 (Sep 13 | Alex)</strong>: Multi-armed 
					bandits with IID rewards.&nbsp;<br>Main reference: chapter 2 of
					<a href="http://slivkins.com/work/MAB-book.pdf">Alex's book 
					draft</a> (revised Sep 15).<br><br>Additional references:<br>
					<ul>
						<li>[Algorithm UCB1] Peter Auer, Nicolo Cesa-Bianchi, 
						and Paul Fischer.&nbsp;<a href="http://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf" rel="nofollow">Finite-time 
						analysis of the multiarmed bandit problem</a>. Machine 
						Learning (2002).</li>
						<ul>
							<li>Chapter 2.2 of<span>&nbsp;</span><a href="http://research.microsoft.com/en-us/um/people/sebubeck/SurveyBCB12.pdf">this 
							survey</a><span>&nbsp;</span>analyzes a slightly more 
							general algorithm.</li>
						</ul>
						<li>[Successive Elimination] Eyal Even-Dar, Shie Mannor 
						and Yishay Mansour.<span>&nbsp;</span><a href="http://www.cs.tau.ac.il/~mansour/papers/06jmlr.pdf">Action 
						Elimination and Stopping Conditions for the Multi-Armed 
						Bandit and Reinforcement Learning Problems</a>. Journal 
						of Machine Learning Research (2006).</li>
					</ul>
					<br><strong>Lecture #3 (Sep 20 | Alex):</strong>&nbsp;Lower 
					bounds on regret<br>Main reference: chapter 3 of
					<a href="http://slivkins.com/work/MAB-book.pdf">Alex's book 
					draft</a> (revised Sep 20).<br><br><br><strong>Lecture #4 (Sep 27 | Alex)</strong> 
					Online learning with experts (i.e., bandits with full 
					feedback &amp; adversarial rewards)<br>Main reference: chapter 6 of
					<a href="http://slivkins.com/work/MAB-book.pdf">Alex's book 
					draft</a> (revised Sep 28).<br><br>Additional reading: for 
					online learning with a very large number of experts, see 
					Chapter 8.4. For bandit problems with structure, see 
					Interlude A.<br><br><strong>Lecture #5 (Oct 4 | Alex)</strong> 
					Adversarial bandits<br>Main reference: chapter 7 of
					<a href="http://slivkins.com/work/MAB-book.pdf">Alex's book 
					draft</a> (revised Oct 5).<br><br><strong>Lecture #6 (Oct 11 | Alekh)</strong> 
					Contextual bandits basics <a href="cb_intro.pdf">Lecture Notes</a>. <a href="cb_intro_slides.pdf">Slides</a> for first part. <br><br><strong>Lecture #7 ( Oct 18 | Alekh)</strong> Off-policy evaluation and learning <a href="off_policy.pdf">Lecture Notes</a>.<br>
					<br><strong>Lecture #8 ( Oct 25 | Alekh)</strong> Exploration in Contextual Bandits <a href="exploration.pdf">Lecture Notes</a> <br>
					<br><strong>Lecture #9 ( Nov 1 | Alekh)</strong> Exploration in Contextual Bandits continued (second half of notes from Lecture 9) <a href="ICML_Tutorial_theme_2.pptx">Slides for second half</a><br><br>
					<strong>Lecture #10 (Nov 8 | Alekh)</strong> Reinforcement Learning <a href="rl_exploration.pdf">Lecture Notes</a><br><br>
					<strong>Lecture #11 (Nov 15 | Alekh)</strong> TBD<br><br>
					<strong>Lecture #12 (Nov 29 | Alex)</strong> Bandits &amp; 
					incentives (tent.)<br><br><strong>Lecture #13 (Dec 6 | Alex)</strong> 
					project presentations<br></span></div>
			</div>
			
			<div id="view3">
            <div class="content">
                
                <p><strong>Sign-up sheet: </strong>Please use
				<a href="https://docs.google.com/spreadsheets/d/11-64UpwUYk6chncVDE_hOgoIU09pZJnVxgmGMiDNcbY/edit?usp=sharing">this sign-up sheet</a> 
				("projects" tab) to help us 
				keep track of the projects. Please update it as appropriate, 
				including your names/emails, brief project description (or the 
				current/tentative version thereof), and project status. Also, 
				use this sheet to to find partners and to be aware of each 
				other's topics and progress.</p>
				<strong>Deliverables &amp; due dates</strong><ol>
					<li><strong>Initial discussion</strong> regarding the possible project 
					topics (probably via email and/or Skype).&nbsp; 
					<p><em>When</em>: the <span class="auto-style3">week of Mon, Oct 23</span> 
					(more details coming up soon).</p>
					<p>See below for advice 
					on <a href="#ChoosingTopic">choosing a topic</a> and
					<a href="#TopicSuggestions">topic suggestions</a>.</p></li>
					<li><strong><a href="#ProjectProposal.">Project proposal</a></strong>: a short document explaining the 
					topic and the proposed approach (<span class="auto-style3">due 
					by the end of Oct 31</span>).</li>
					<li><strong>Mid-project discussion</strong>: optional but 
					encouraged, especially if you are doing simulations or 
					research. (<span class="auto-style3">Week of Mon, Nov 13</span> 
					-- sign-up sheet will be up.)</li>
					<li><strong><a href="#FinalReport.">Final report</a></strong>: a short (3-7 pp long) academic-style 
					paper describing the background and what you've done for the 
					project. (<span class="auto-style3">Tentative deadline: Dec 5</span>).</li>
					<li><strong><a href="#ProjectPresentation.">Final presentation</a></strong>: 
					A presentation during the last class (Dec 6).</li>
				</ol>
				<p>Please do not hesitate to send us questions at any point. </p>
				<p><strong>Our crew.</strong> The projects will be supervised by 
				the two instructors, and two more people:
				<a href="http://www.cis.upenn.edu/~wuzhiwei/">Steven Wu</a> and
				<a href="https://sites.google.com/a/umich.edu/nanjiang/">Nan 
				Jiang</a>. There will be one supervisor for each project, to 
				which all project-related communication should be directed.&nbsp;&nbsp;&nbsp; </p>
				<p><strong><a name="ChoosingTopic">Choosing a topic</a>.</strong>&nbsp; The project topic can be 
				anything directly related to bandits, contextual bandits, or RL, 
				subject to our approval. (If you are 
				unsure whether a given topic is OK,&nbsp; please 
				ask.) Generally, there are three types of projects:</p>
				<ol>
					<li><em>Reading (literature review):</em> read up on a given 
					sub-topic.&nbsp; What is are the problem formulations and 
					how they are motivated? What are the main results and the 
					crucual techniques to obtain these results? What are the 
					open questions? What are the papers in this line of work, 
					and who did what?</li>
					<li><em>Experiments: </em>&nbsp;pick a sub-topic and several 
					algorithms for this sub-topic, run experiments with these 
					algorithms to see which ones are better (for which 
					"regimes"). You can use simulated data and/or some of the 
					real datasets. </li>
					<li><em>Original research:</em> pick an open problem and try 
					to make a progress on it. Could be theoretical and/or 
					experimental, up to you. Such a project is likely 
					to involve some amount of reading, to figure out what is 
					known and what is likely to help. It is great if by the end 
					of the class you make a substantial progress towards a 
					future publication, but it is totally OK to only have 
					preliminary results, or no results at all. (After all, good 
					research is rarely done in a few weeks.)&nbsp; </li>
				</ol>
				<p>These three types are not mutually exclusive: an original 
				research may involve much reading; and reading- 
				and experimets- type projects may be driven by a 
				specific research question, and/or may help towards research 
				projects in the future. </p>
				<p>To keep #projects managable, we kindly ask you to work
				<span class="auto-style3">in&nbsp; groups of three of more. </span>
				We may allow, at our discretion, a few groups of two for some 
				simulations or research projects. Working in groups tends to be 
				more productive, more fun and also a useful collaborative 
				experience. </p>
				<p>If you are working on a course-related research project with 
				some collaborators who are not attending this course, you are 
				encouraged to submit an account of this research project as the 
				"class project". But please ask one of us first.</p>
				<strong><a name="ProjectProposal.">Project proposal.</a></strong> For a reading project, 
				mention a few papers to start with. For simulations, mention a 
				few algorithms, a few types of problem instances you are 
				planning to consider (if doing simulations), and the data 
				sources you are planning to use (if working with real data). For 
				an open research project, describe the open problem and how you 
				plan/hope to go about solving it. While a project may evolve and morph compared to the 
				proposal, a good proposal&nbsp; usually helps to organize your 
				thinking.<br><br><em>NB:</em> It is OK if an original research 
				project evolves into a reading and/or experiments project and 
				vice versa. However, please keep us posted. <br><br><em>NB:</em> 
				Sometimes a bigh chunk of the overall work for the project 
				happens <em>before (and while)</em> you write the project 
				proposal. So, don't be alarmed if generating a project proposal 
				takes much work, this probably means that you'd have less work 
				to do later!<br><br>A page of text should usually suffice, possibly 
				less. Please use LaTeX, and insert bibliography as appropriate. 
				Send the PDF to
				<a href="mailto:bandits-fa17-instr@microsoft.com">
				bandits-fa17-instr@microsoft.com</a>, with a subject like "project proposal 
				(project name)". CC all collaborators, and Steve or Nan or they 
				are involved. <br> <br>
				
				Both in the email and in the PDF, please include the following, 
				for each person in the group: full name, UNI, 
				PhD/year or Masters/year, department/school, and a few words on&nbsp; background and interests.
				<br>

				
				<br>
				
				<strong>
				<a name="TopicSuggestions">Topic suggestions</a>.</strong> <em>[We 
				may modify these over time... ]</em>
				<ul>
					<li><span class="auto-style5"><em>[reading: bandits with auxiliary 
					structure]</em></span> much literature on bandits assumes 
					auxiliary structure, compared to the "standard" bandit 
					model. Algorithms can take advantage of such structure, 
					e.g., to cope with a huge #actions (cf.&nbsp; Interlude A in 
					Alex's book). Examples include:<ul>
						<li>a Bayesian prior, either generic (e.g., see Chapter 
						4 of Alex's book) or specific (e.g., 
						<a href="https://arxiv.org/abs/0912.3995">bandits with a Gaussian Process prior</a>).</li>
						<li>the function from arms to expected rewards has some 
						nice shape: e.g., linear, convex, or Lipschitz. (e.g.: <a href="http://www.jmlr.org/proceedings/papers/v15/chu11a/chu11a.pdf">LinUCB</a>, <a href="https://arxiv.org/abs/cs/0408007"> Bandit Gradient Descent</a>, 
						ch. 5 of Alex's book)</li>
					</ul>
&nbsp;Take one 
					type of structure (e.g., linear bandits), and read up on it. 					</li>
					<li><span class="auto-style5"><em>[reading &amp; simulations: 
					bandits with auxiliary data]</em></span> In many scenarios, 
					algorithm has access to extra data, compared to the 
					"standard" bandit model. How to leverage this data? For 
					example:<ul>
						<li>after each round, rewards for several arms may be 
						revealed, not just for the chosen arm. (e.g.: <a href="https://arxiv.org/abs/1409.8428"> Bandits with feedback graphs</a>)</li>
						<li>some exploration data may be available before the 
						algorithm starts,&nbsp; (e.g. <a href="http://papers.nips.cc/paper/3977-learning-from-logged-implicit-exploration-data.pdf"> Exploration Scavenging</a>)  </li>
						<li>some exploration may be happening anyway, in 
						parrallel with the algorithm (e.g., because different 
						people just try out different things).</li>
					</ul>
					</li>
					<li><span class="auto-style5"><em>[reading and simulations: bandits with 
					structured actions]</em></span> in many applications, 
					actions have a specific structure, e.g., action is a slate 
					of search results. There are several ways to model this, 
					depending on:<ul>
						<li>how users react to the slate E.g., click once and 
						leave, or consider each item no matter what? (e.g.: <a href="http://www.jmlr.org/proceedings/papers/v48/lif16-supp.pdf"> Cascading models</a>)</li>
						<li>what is the reward metric? E.g., does it help to 
						have <em>two</em> clicks on the slate?</li>
						<li>what feedback is observed? E.g., all clicks, or only 
						the total reward? (e.g.: <a href="http://arxiv.org/abs/1605.04812"> Slates</a>, <a href="http://arxiv.org/abs/1502.05890"> Semibandits</a>)</li>
						<li><span class="auto-style5"><em>[simulations]</em></span> Take a real-world ranking dataset, such as from <a href="https://www.microsoft.com/en-us/research/project/mslr/">MSLR</a>. Simulate different user models on this data, and evaluate the effect of an algorithm tailored to that model, an algorithm just operating on global rewards of the slate, and a greedy algorithm.
					</ul>
					This has been studied both with and without contexts. <span class="auto-style6"><em>(Current interests: Alekh.)</em></span></li></li>
					<li><span class="auto-style5"><em>[reading: general techniques]</em></span> 
					same technique may 
					appear in many settings, from bandits to contextual bandits 
					to RL.&nbsp; Prominent examples:&nbsp;
					<ul>
						<li>multiplicative weights (e.g., see
						<a href="http://theoryofcomputing.org/articles/v008a006/">
						this survey</a>).</li>
						<li>follow the perturbed/reguralized 
					leader (e.g., see ch. 8 from Alex's book).</li>
						<li>UCB / optimism under uncertainty (e.g., see ch. 5 
						and ch. 10 in Alex's book for applications beyond the 
						basic bandit model).</li>
						<li>non-adaprive exploration (explore-first, 
						epsilon-greedy, etc.)</li>
						<li>Thompson Sampling (start with ch. 4 of Alex's book).</li>
					</ul>
					</li>
					Take one such technique and read up on 
					it.<li><span class="auto-style5"><em>[simulations: Thompson 
					Sampling]</em></span> a famous bandit algorithm called 
					"Thompson Sampling" relies on exactly sampling from Bayesian 
					posteriors on mean rewards. Whereas in many practical 
					applications one can only sample approximately. What is the 
					effect of approximate sampling on algorithm's performance?</li>
					<li><span class="auto-style5"><em>[bandits &amp; social learning]</em></span> As people make 
					decisions over time (e.g., where to eat today?) they utilize 
					information revealed by others in the past (think Yelp) and 
					produce information that may help others in the future 
					(e.g., leave a review). Do they explore enough for the 
					common good? How to incentivize them to explore more? What 
					if a bandit algorithm issues recommendations, but people do 
					not have to follow them?  
					<ul>
						<li><span class="auto-style5"><em>[reading]</em></span>&nbsp; 
						start with <a href="http://dl.acm.org/authorize?N47974">this informal article</a> and this tutorial 
						<span>(</span><a href="http://slivkins.com/work/ec17-tutorial-Alex.pdf">part 
						I</a><span>&nbsp;and&nbsp;</span><a href="http://slivkins.com/work/ec17-tutorial-Bobby.pdf">part 
						II</a><span>).</span></li>
						<li><span class="auto-style5"><em>[simulations]</em></span> see what happens under 
						different models of human behavior, and different ways 
						to present or censor past observations. Such simulations 
						would plug nicely into one of our active research 
						projects. </li>
					</ul>
					<span class="auto-style6"><em>(Current interests: Alex, 
					Steven.)</em></span></li>
					<li><em><span class="auto-style5">[reading: bandits &amp; fairness]</span>
					</em>bandit algorithms 
					often make choices <em>for</em> people (e.g., web search 
					results) or <em>among</em> people (whom to give a loan?). 
					Are some individuals or groups treated unfairly? What 
					"fairness" means, and how to ensure it? (e.g., see
					<a href="http://www.cis.upenn.edu/~aaroth/fairbandits.html">this paper</a> and
					<a href="http://www.cis.upenn.edu/~aaroth/incentfairness.html">that paper</a>).<br>
					<span class="auto-style6"><em>(Current interests: Alekh, 
					Alex, Steven.)</em></span></li>
					<li><span class="auto-style5"><em>[online learning &amp; 
					games]</em></span> what if an online learning algorithm is playing a game against an adversary or 
						another algorithm? What does this process converge to, 
					and how fast?
					<ul>
						<li><span class="auto-style5"><em>[reading]</em></span> 
						Start with
						<a href="http://www.cs.cornell.edu/courses/cs683/2007sp/lecnotes/week2.pdf">this lecture note</a> and
						<a href="http://rob.schapire.net/papers/FreundScYY.pdf">that paper</a>.</li>
						<li><span class="auto-style5"><em>[simulations]</em></span> 
						what if the algorithm is best-responding to the 
						"empirical play" (time-averages) of the adversary?
						<span class="auto-style6"><em>(Current interests: 
						Steven)</em></span></li>
					</ul>
					</li>
					<li><span class="auto-style5"><em>[reading: online learning &amp; 
					economic mechanisms]</em></span> what if a bandit / online 
					learning algorithm 
					...</li>
					<ul>
						<li>... is used in an auction, either by the auction 
						maker or by auction participants? (e.g., see 						
						<a href="http://cesa-bianchi.di.unimi.it/Pubblicazioni/secondPrice.pdf">this paper</a> and
						<a href="https://arxiv.org/abs/1611.01688">that paper</a>)</li>
						<li>... is used to determine prices &amp; offers for sale? 
						(e.g., see <a href="https://arxiv.org/abs/1108.4142">this paper</a> and references therein).</li>
						<li>... determines who does what and at which price at a 
						crowdsourcing market? (e.g., see 						
						<a href="http://arxiv.org/abs/1308.1746">this survey</a>)</li>
					</ul>
					<span class="auto-style6"><em>(Current interests: Alex)</em></span>&nbsp;
					<li><span class="auto-style5"><em>[reading, simulations and research: contextual 
					bandits theory]</em></span> Possible topics include:<ul>
						<li><span class="auto-style5"><em>[reading]</em></span> beyond IID: adversarial contexts and/or rewards (e.g.: 
						<a href="https://arxiv.org/abs/1606.00313">BISTRO+</a> and predecessors).</li>
						<li><span class="auto-style5"><em>[reading]</em></span> algorithms that adapt to data, achieving better 
						performance for "nice" problem instances (e.g.: 
						<a href="http://www.szit.bme.hu/~oti/publications/hannan.pdf">first-order regret bounds</a>, 
						<a href="http://proceedings.mlr.press/v65/agarwal17a.html">open problem</a>, 
						<a href="https://arxiv.org/abs/1612.06246">CORRAL</a>).</li>
						<li><span class="auto-style5"><em>[simulations]</em></span> How do the adversarial algorithms behave on i.i.d. contexts and rewards, compared with algorithms for those settings? How do the i.i.d. algorithms work on non-i.i.d. and adversarial instances? Which seems more practically useful?</li>
						<li><span class="auto-style5"><em>[research]</em></span> Study the 
						<a href="http://cs.bme.hu/%7Egergo/files/NB15.pdf">implicit exploration technique</a> and try combining it with efficient contextual bandit methods as a replacement for uniform exploration? Does this lead to improvements in a structured action setting? You can also empirically evaluate the resulting approach in lieu of theory, or both.</li>
					</ul>
					<span class="auto-style6"><em>(Current interests: Alekh)</em></span>&nbsp;
					<li><span class="auto-style5"><em>[reading and simulations: contextual 
					bandits applications and deployments]</em></span> Possible topics include:<ul>
						<li><span class="auto-style5"><em>[reading]</em></span> Contextual bandits in the wild: deployed systems and learnings (e.g.: 
						<a href="http://arxiv.org/abs/1606.03966">Decision Service paper</a>, 
						<a href="https://research.google.com/pubs/pub43146.html">Technical debt paper</a>, 
						<a href="http://hunch.net/~rwil/">ICML Tutorial</a>)</li>
						<li><span class="auto-style5"><em>[reading/simulations]</em></span> Contextual bandit applications ( See for a partial ref of application papers: 
						<a href="http://hunch.net/~rwil/Bibliography.pptx">ICML Tutorial</a>. Pick one or more papers around an application, and run simulations with some algorithms you have learned in this course on that application.</li>
					</ul>
					<span class="auto-style6"><em>(Current interests: Alekh)</em></span>&nbsp;
					<li><span class="auto-style5"><em>[reading and research: Off-policy evaluation]</em></span> How to best evaluate policies given logged data?<ul>
						<li><span class="auto-style5"><em>[reading]</em></span> Biased estimators (e.g.: 
						<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/paper-14.pdf">doubly robust with bias</a>, 
						<a href="https://arxiv.org/abs/1612.01205">SWITCH</a>)</li>
						<li><span class="auto-style5"><em>[reading]</em></span> Applications in Information Retrieval (see e.g.: 
						<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/ftir-online-evaluation-final-journal.pdf">Hofmann et al. monograph</a>)</li>
						<li><span class="auto-style5"><em>[research]</em></span> How to improve our evaluation approach for exploration algorithms using biased algorithms like SWITCH? Can you analyze in theory if the exploration algorithm has some nice properties? (a starting point for such a property might be the structural assumption in the 
						<a href="https://arxiv.org/abs/1612.06246">CORRAL </a>paper).</li>
						<li><span class="auto-style5"><em>[research]</em></span> How to extend the minimax lower bounds from the SWITCH paper for structural conditions on rewards such as linear?</li>
					</ul>
					<span class="auto-style6"><em>(Current interests: Alekh)</em></span>&nbsp;
					<li><span class="auto-style5"><em>[reading, simulations and research: Off-policy evaluation]</em></span> How to best learn good policies given logged data?<ul>
						<li><span class="auto-style5"><em>[reading]</em></span> Reducing the variance in off-policy learning (see e.g.<a href="https://www.microsoft.com/en-us/research/publication/self-normalized-estimator-counterfactual-learning/"> Self-normalized poem</a>)</li>
						<li><span class="auto-style5"><em>[reading]</em></span> Can you extend the IPS-based off-policy learning ideas to further incorporate a variance term like the work above? You might get some inspiration from the 
						<a href="http://proceedings.mlr.press/v15/beygelzimer11a.html">EXP4.P</a> paper. 
						</li>
						<li><span class="auto-style5"><em>[reading]</em></span> Study the effect of using different off-policy learning methods inside your favorite contextual bandit algorithm (such as to train the greedy policy in epsilon-greedy).</li>
					</ul>
					<span class="auto-style6"><em>(Current interests: Alekh)</em></span>&nbsp;
					  <li><span class="auto-style5"><em>[reading, simulations and research: Exploration in RL]</em></span> Read on and experiment with different exploration algorithms for reinforcement learning<ul>
						  <li><span class="auto-style5"><em>[reading]</em></span> Exploration in small-state MDPs (e.g.: 
						  <a href="http://web.eecs.umich.edu/~baveja/Papers/MLjournal6.pdf">E<sup>3</sup></a>, 
						  <a href="http://www.jmlr.org/papers/v3/brafman02a.html">R-MAX</A>, 
						  <a href="http://www.jmlr.org/papers/v11/jaksch10a.html">UCRL</a>, 
						  <a href="http://www.hunch.net/~jl/projects/RL/Delayed_Q/icml06.pdf">Delayed-Q learning</a>)</li>
						  <li><span class="auto-style5"><em>[reading]</em></span> Exploration with complex observations (e.g.: 
						  <a href="https://papers.nips.cc/paper/5245-model-based-reinforcement-learning-and-the-eluder-dimension.pdf">Eluder dimension</a>, 
						  <a href="https://arxiv.org/abs/1610.09512">Bellman Rank</a>, 
						  <a href="http://pathak22.github.io/noreward-rl/resources/icml17.pdf">Curiosity driven exploration</a>) 
						  </li>
						  <li><span class="auto-style5"><em>[simulations]</em></span> Take some standard RL benchmark problems where exploration matters. Compare an algorithm like R-MAX or UCRL with a (contextual) bandit algorithm. Can you try the same for an Atari game?</li>
						  <li><span class="auto-style5"><em>[research]</em></span> Come up with a theoretical model to analyze an algorithm like the curiosity-driven exploration paper. Hint: try to capture the idea that only some aspects of an observation might matter to the optimal policy/value function given a task.</li>
					  </ul>
					  <span class="auto-style6"><em>(Current interests: Alekh, Nan)</em></span>&nbsp;
					    <li><span class="auto-style5"><em>[reading: Predictive State Representations]</em></span> What constitutes a state in RL? Is there a procedural way to define states in RL? PSRs provide one such notion (e.g.: 
						<a href="http://web.eecs.umich.edu/~baveja/Papers/psr.pdf">Original PSR paper</a>, 
						<a href="https://www.cc.gatech.edu/~bboots3/files/RSSclosingtheloop.pdf">Spectral learning for PSRs</a>)</li>
					    <span class="auto-style6"><em>(Current interests: Alekh, Nan)</em></span>&nbsp;
					    <li><span class="auto-style5"><em>[reading: Hierarchical Reinforcement Learning]</em></span> If the planning horizon is long, and the task you are trying to accomplish has re-usable subtasks, can you effectively leverage this structure? (see e.g. 
						<a href="http://web.eecs.umich.edu/~baveja/Papers/aij.pdf">Options paper</a>, 
						<a href="http://www.ifaamas.org/Proceedings/aamas2011/papers/A6_R70.pdf">General value functions</a> and 
						<a href="https://arxiv.org/abs/1706.04208">application to Atari</a>)</li>
					    <span class="auto-style6"><em>(Current interests: Alekh, Nan)</em></span>&nbsp;
					    <li><span class="auto-style5"><em>[reading/simulations: Proper evaluation of RL algorithms]</em></span> The training-validation-test paradigm in supervised learning is robust to many types of experimental errors. We have repositories of standardized dataset splits, which makes it easier to compare experiments across papers. RL agents generate their own data, and are often run in simulators which makes it harder to do experiments which can be compared. How to make sure that our results are genuinely interesting? Do you want to reproduce the results of your favorite empirical RL paper according to a recommended methodology? (see e.g.: 
						<a href="https://arxiv.org/abs/1709.06560">Deep RL that matters</a>, 
						<a href="https://arxiv.org/abs/1709.06009">Atari evaluation paper</a>)</li>
					<li><span class="auto-style5"><em>[reading: Policy improvement and gradient]</em></span></li> Given a starting policy supplied by an expert at training time, how does an RL agent learn a policy as good as it or better? Can the agent bootstrap from an expert and continue to get even better over time?<ul>
						<li><span class="auto-style5"><em>[reading]</em></span> Policy improvement approaches (see e.g.: 
						<a href="http://ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdfhttp://ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf">DAGGER</a>, 
						<a href="http://icml.cc/2015/tutorials/AdvancesStructuredPrediction.pdf">ICML tutorial</a>, 
						<a href="http://ri.cmu.edu/wp-content/uploads/2017/04/DifferentiableAggrevate-11.pdf">Deeply Aggrevated</a>) 
						</li>
						<li><span class="auto-style5"><em>[reading]</em></span> Policy search and gradient approaches (quite related to the one above, you might want to pick a few from both) (see e.g.: 
						<a href="http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Policy gradient paper</a>, 
						<a href="http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf">TRPO</a>)</a>
                                           </ul>
					</li>
				</ul>
                                               
				<p><em><strong><a name="FinalReport.">Final report.</strong>&nbsp; </em>
				The final report should 
				clearly and concisely describe the project: what was the chosen 
				topic, why it is interesting, what you've set out to accomplish, 
				what you've learned/understood/accomplished, and what it means 
				in a broader context. <br><br>The report should resemble a 
				research paper in terms of style and presentation. Try to make 
				it accessible to students in this course: explain things 
				clearly, and try not assume background beyond what was covered 
				in class.</p>
				<ul>
					<li>"Introduction" section should state what is the 
					topic/problem, why it is interesting, background and [some] 
					prior work with appropriate citations, what you've set out 
					to accomplish, and (on a high level) what you've 
					accomplished.</li>
					<li>"Preliminaries<em>"</em> section is a place to introduce 
					models and/or notation. Also, if you'll be using tools that 
					are somewhat "advanced" for this course (e.g., martingales, 
					linear duality, KL-divergence) mention them here, and 
					perhaps explain briefly what it is and how you'd be using 
					it. </li>
					<li>"Body" of the report presents all the details.</li>
				</ul>
				<p>For new results: explain what the results <em>mean,</em> be 
				as explicit as possible about why they are interesting given the 
				prior work.&nbsp; Include enough detail: </p>
				<ul>
					<li>for theory: precise 
				theorem statements, and full proofs, or at least proof sketches 
				that contain the main ideas, </li>
					<li>for simulations/experiments: 
				detailed experimental setup, algorithms used, and what was 
				observed. </li>
				</ul>
				<p>If you tried some approach and it didn't work out, you can write 
				about it, too! Try to explain why it didn't work and what are 
				the obstacles; present lessons / conclusions, should you have 
				any.&nbsp; </p>
				<p>For reading projects: what is the goal of your project? 
				Clearly state and cite the papers you've looked at. Try to state 
				the main results as precisely and explicitly as possible (but 
				also try to be succinct, and do use informal descriptions if 
				needed). Explain what these results mean. What are the main 
				techniques to achieve them. Ideally, also what are the cool open 
				problems in the area, and why prior work falls short in solving 
				them.</p>
				<p>Mapping out the relevant work is an imporant part of the 
				project. In particular, when citing a line of work, cite several 
				papers, not just one; ideally, the initial paper or two, and a 
				few of the most important follow-ups. Cite surveys whenever 
				applicable. DBLP is a good tool to track down correct references 
				(e.g., do not cite an arxiv version if there is a conference 
				publication), and also to get BibTeX entries.</p>
				<em>Formalities.</em> The document should be 3-5 pp long, not 
				including fugures and references, in 11pt font with 1-inch 
				margins and single spacing. Include full names and emails for 
				all authors. LaTeX strongly preferred (but do talk to me if this 
				is a problem). Then the desired formatting can be achieved by&nbsp;
				<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
				\documentclass[11pt,letterpaper]{article}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
				\usepackage{fullpage}.<br>BibTeX is recommended for 
				bibliography. <br><br><em>Submission.</em> Submit PDF file by 
				email to<br><a href="mailto:bandits-fa17-instr@microsoft.com">
				bandits-fa17-instr@microsoft.com</a>, with a subject like "final 
				project report (supervisor: name1, name2, name3)". <br><br><strong>
				<a name="ProjectPresentation.">Project 
				presentation.</a></strong> Each project will need to make a 
				presentation in the final class, via a (very) short talk or a 
				poster. We will provide a more specific plan once we see how 
				many projects we have.
				<p>&nbsp;</p>
                
                <p>&nbsp;</p>
			</div>
			</div>
			
			<div id="view4">
            <div class="content">
                
                <strong>Homeworks are for self-study only</strong>: they will 
				not be not collected or graded. However, we will be happy to 
				discuss the solutions in office hours and/or on Piazza. <br><br>
				<a href="hw1.pdf">Homework 1</a>: out Sep 21. <br><br>
				<a href="hw2.pdf">Homework 2</a>: out Oct 6.
			</div>
			</div>

        </div>
    </div>
</body>
</html>
